{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ChcjeANu56pc"
   },
   "source": [
    "#### Install mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "9vGUsbsQ4cKd",
    "outputId": "06c95dfa-f4d9-415c-df33-5356210b4d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n"
     ]
    }
   ],
   "source": [
    "# install \"swig\" if can not build eggs for mecab-python3\n",
    "!sudo apt-get install mecab libmecab-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "aET_pOu64hX-",
    "outputId": "05eaef6d-86d4-4a2d-bbae-74ef83a4986c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mecab-python3\n",
      "  Using cached https://files.pythonhosted.org/packages/ac/48/295efe525df40cbc2173748eb869290e81a57e835bc41f6d3834fc5dad5f/mecab-python3-0.996.1.tar.gz\n",
      "Collecting pysummarization\n",
      "  Downloading https://files.pythonhosted.org/packages/17/50/2b70caf3092c8b6f01022ba0adff9312fc3172be59322d60857a00bf3cab/pysummarization-1.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from pysummarization) (3.2.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pysummarization) (1.14.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->pysummarization) (1.11.0)\n",
      "Building wheels for collected packages: mecab-python3\n",
      "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/73/71/4f/63a79925b5e9bb38932043917cc60140beb8022ac14a952b1e\n",
      "Successfully built mecab-python3\n",
      "Installing collected packages: mecab-python3, pysummarization\n",
      "Successfully installed mecab-python3-0.996.1 pysummarization-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3 pysummarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRXJiAKV6UFm"
   },
   "source": [
    "#### Import libraby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4Ootu53417S"
   },
   "outputs": [],
   "source": [
    "from pysummarization.nlpbase.auto_abstractor import AutoAbstractor\n",
    "from pysummarization.tokenizabledoc.mecab_tokenizer import MeCabTokenizer\n",
    "from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncH0RYKM60es"
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kr1D2wP47_f"
   },
   "outputs": [],
   "source": [
    "document =  \"\"\"２月２７～２８日の米朝首脳会談で、トランプ米大統領が北朝鮮の金正恩キムジョンウン朝鮮労働党委員長に日本人拉致問題を提起したのは、初日に行われた１対１の会談の冒頭だったことがわかった。\n",
    "\n",
    "　複数の日本政府関係者が明らかにした。\n",
    "\n",
    "　安倍首相は２月２０日のトランプ氏との電話会談で、拉致問題を正恩氏に提起するよう要請した。トランプ氏の発言は、これに配慮したものとみられる。正恩氏は核・ミサイル問題が最初の議題と想定していたのか、その場で「驚いた表情」を見せたという。\n",
    "\n",
    "　トランプ氏は１対１の会談に続き、２７日の夕食会でも拉致問題を取り上げた。日本政府は「首相の注文通り」（関係者）と歓迎している。政府は日米連携をテコに日朝の首脳による直接対話につなげ、拉致問題を打開したい考えだ。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8gQkSTHjhrl"
   },
   "outputs": [],
   "source": [
    "document = \"\"\"「レオパレス21」の建物で施工不備が相次いでいる問題で、外部の弁護士による調査委員会がまとめた中間報告が明らかになりました。当時の社長の指示のもと外壁などに設計図と異なる材料を使う方向性が示されたとしたうえで、工期の短縮を求められていたことが背景にあったとしています。\n",
    "\n",
    "レオパレス21が平成５年から13年にかけて建設した建物では、部屋を区切る壁や外壁に設計図と異なる材料を使い、耐火構造が法律の基準を満たさないなどの不備が相次いで明らかになり、特に危険性の高い641棟の入居者およそ7700人に転居を求める事態となっています。\n",
    "\n",
    "会社は、外部の弁護士３人による調査委員会を設けて、原因究明を進め中間報告を取りまとめました。\n",
    "\n",
    "それによりますと、設計や開発の担当者へのヒアリングなどから、平成18年５月までトップをつとめていた当時の社長の指示のもと工事の効率化などを目的として設計図と異なる材料を使う方向性が示されたとしています。\n",
    "\n",
    "そのうえで背景には、学生や社会人の新生活が始まるタイミングに建物の完成が間に合うよう工期の短縮が求められていたことが、大きく関係していたとみられると指摘しています。\n",
    "\n",
    "そして、「不備の原因・背景となる問題は会社の一部の部署にとどまるものではなく組織的・構造的に存在していた」と結論づける一方で、「意図を持って組織的に行われていたかどうかは、さらに調査が必要だ」としています。\n",
    "\n",
    "調査委員会は、こうした内容をすでに国土交通省に報告していて、18日午後、正式に発表することにしています。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jz9MrLkGkigw"
   },
   "outputs": [],
   "source": [
    "listDoc = [sentence for sentence in document.split('。') if sentence]\n",
    "doc_len = len(listDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "nuePvT7N5IDN",
    "outputId": "d4279d58-9425-45a3-9e01-a79dd960e9e9"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a475d1185cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mabstractable_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_top_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Summarize document.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_abstractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractable_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Output result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pysummarization/nlpbase/auto_abstractor.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, document, Abstractor, similarity_filter)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mnormalized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_filter_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pysummarization/nlp_base.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         '''\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizable_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlistup_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pysummarization/tokenizabledoc/mecab_tokenizer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, sentence_str)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         '''\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-Owakati\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mwordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/MeCab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# summarize with ratio around 1/3\n",
    "n_sentence = (doc_len+2)//3 # upper integer of len/3\n",
    "\n",
    "# Object of automatic summarization.\n",
    "auto_abstractor = AutoAbstractor()\n",
    "# Set tokenizer for Japanese(Mecab).\n",
    "auto_abstractor.tokenizable_doc = MeCabTokenizer()\n",
    "# Set delimiter for making a list of sentence.\n",
    "auto_abstractor.delimiter_list = [\"。\", \"\\n\"]\n",
    "# Object of abstracting and filtering document.\n",
    "abstractable_doc = TopNRankAbstractor()\n",
    "abstractable_doc.set_top_n(n_sentence)\n",
    "# Summarize document.\n",
    "result_dict = auto_abstractor.summarize(document, abstractable_doc)\n",
    "\n",
    "# Output result.\n",
    "for sentence in result_dict[\"summarize_result\"]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeU_aowfp1Ep"
   },
   "outputs": [],
   "source": [
    "sumText = \"\"\n",
    "for sentence in result_dict['summarize_result']:\n",
    "    sumText += sentence"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ChcjeANu56pc"
   ],
   "name": "pysum.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
